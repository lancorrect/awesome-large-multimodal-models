# Awesome large multimodal models

Recently, with the huge development of large language model (LLM), there were a plenty of multi-modal works employing LLM to leverage the linguistic knowledge. These works introduced a new paradigm in multimodal field and achieved wonderful results.

This repo is a collection of recent multimodal researches with LLM and will be updated continuously.

**Welcome to follow and star!**

## Table of Contents

[Surveys](#surveys)

[Image-to-text generation](#image-to-text-generation)

[Multimodal alignment](#multimodal-alignment)

[Multimodal instruct tuning with machine-generated instruction-following data](#multimodal-instruct tuning-with-machine-generated-instruction-following-data)

[More multimodal method](#more-multimodal-method)

[Application](#application)

[Multimodal Aspect-based Sentiment Analysis](#multimodal-aspect-based-Sentiment-Analysis)

[Surveys](#surveys)

[Alignment](#alignment)

[Translation](#translation)

[Cross attention w/o alignment or translation](#cross-attention-wo-alignment-or-translation)

## Surveys

Multimodal Foundation Models- From Specialists to General-Purpose Assistants [Arxive 2023] [[paper](https://arxiv.org/pdf/2309.10020.pdf)]

## Image-to-text generation

VisualGPT: Data-efficient Adaptation of Pretrained Language Models for Image Captioning [CVPR 2022] [[paper](https://openaccess.thecvf.com/content/CVPR2022/html/Chen_VisualGPT_Data-Efficient_Adaptation_of_Pretrained_Language_Models_for_Image_Captioning_CVPR_2022_paper.html)] [[github](https://github.com/Vision-CAIR/ VisualGPT.)]

## Multimodal alignment

BLIP-2: Bootstrapping Language-Image Pre-training  with Frozen Image Encoders and Large Language Models [ICML 2023] [[paper](https://openreview.net/pdf?id=KU9UojoX7U)] [[github](https://github.com/salesforce/LAVIS/tree/main/projects/blip2)]

BLIP: Bootstrapping Language-Image Pre-training for  Unified Vision-Language Understanding and Generation [ICML 2022] [[paper](https://proceedings.mlr.press/v162/li22n/li22n.pdf)] [[github](https://github. com/salesforce/BLIP)]

Align before Fuse: Vision and Language Representation Learning with Momentum Distillation [NeurIPS 2021] [[paper](https://proceedings.neurips.cc/paper_files/paper/2021/file/505259756244493872b7709a8a01b536-Paper.pdf)] [[github](https://github.com/salesforce/ALBEF)]

## Multimodal instruct tuning with machine-generated instruction-following data

MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models [Arxive 2023] [[paper](https://arxiv.org/pdf/2304.10592.pdf)] [[github](https://github.com/Vision-CAIR/MiniGPT-4)]

Visual Instruction Tuning [Arxive 2023] [[paper](https://arxiv.org/pdf/2304.08485.pdf)] [[github](https://llava-vl.github.io)]

## More multimodal method

NExT-GPT: Any-to-Any Multimodal LLM [Arxive 2023] [[paper](https://arxiv.org/pdf/2309.05519.pdf)] [[github](https://next-gpt.github.io/)]

## Application

### Multimodal Aspect-based Sentiment Analysis

#### Surveys

A Survey on Aspect-Based Sentiment Analysis: Tasks, Methods, and Challenges [IEEE Transactions on Knowledge and Data Engineering 2022] [[paper](https://ieeexplore.ieee.org/abstract/document/9996141/)]

#### Alignment

Joint Multi-modal Aspect-Sentiment Analysis with Auxiliary Cross-modal Relation Detection [EMNLP 2021] [[paper](https://aclanthology.org/2021.emnlp-main.360.pdf)] [[github](https://github.com/MANLP-suda/JML)]

MASAD: A large-scale dataset for multimodal aspect-based sentiment analysis [Neurocomputing 2021] [[paper](https://www.sciencedirect.com/science/article/abs/pii/S0925231221007931)] [[github](https://github.com/DrJZhou/MASAD)]

#### Translation

Exploiting BERT For Multimodal Target Sentiment Classification Through Input Space Translation [[MM '21: Proceedings of the 29th ACM International Conference on Multimedia](https://dl.acm.org/doi/proceedings/10.1145/3474085)] [[paper](https://dl.acm.org/doi/abs/10.1145/3474085.3475692)] [[github](https://github.com/codezakh/exploiting-BERT-thru-translation)]

#### Cross attention w/o alignment or translation

Adapting BERT for Target-Oriented Multimodal Sentiment Classification [IJCAI 2019] [[paper](https://www.ijcai.org/Proceedings/2019/0751.pdf)] [[github](https://github.com/jefferyYu/TomBERT.git)]

## License

Awesom large multimodal models is released under the Apache 2.0 license.

## Contact

Please feel free to contact me with email zhihaolancorrect@gmail.com

or

Open an issue that let me know.